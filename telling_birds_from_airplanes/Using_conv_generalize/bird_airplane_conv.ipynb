{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528e05d8",
   "metadata": {},
   "source": [
    "## Telling birds from airplanes using conv\n",
    "For our CIFAR-10 data, we will resort to nn.Conv2d. At a minimum, the arguments we provide to nn.Conv2d are the number of input features, the number of output features, and the size of kernel. For instance, for our first convolutional module, we'll have 3 input features per pixel (RGB) and an arbitrary number of channels in the output-say, 16. The more channels in the output image, the more the capacity of the network. We need the channels to be able to detect many different types of features. Also, because we are randomly initializing them, some of the features we'll get, even after training, will turn out to be useless. Let's stick to a kernel size $3\\times 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0e93f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fae53e",
   "metadata": {},
   "source": [
    "For a single output pixel value, our kernel would consider, say in\\_ch=3 input channels, so the weight component for a single output pixel value is of shape $in\\_ch\\times 3\\times 3$. Finally, we have as many of those as we have output channels, here out\\_ch=16, so the complete weight tensor is $out\\_ch\\times in\\_ch \\times 3\\times 3$, in our case, $16\\times 3\\times 3\\times 3$. The biase will have size 16. Let's verify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a115012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f4f48",
   "metadata": {},
   "source": [
    "A 2D convolution pass produces a 2D image as output, whose pixels are a weighted sum over neighborhoods of the input image. In our case, both kernel weights and the bias conv.weight are initialized randomly, so the output image will not be particularly meaningful. As usual, we need to add the zeroth batch dimension with unsqueeze if we want to call the conv module with one input image, since nn.Conv2d expects a $B\\times C\\times H\\times W$ shaped tensor as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b6d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "data_path = '../../dlp_es/CIFAR-10/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                              (0.2470, 0.2435, 0.2616))]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                                  (0.2470, 0.2435, 0.2616))]))\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e6ba2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2391534",
   "metadata": {},
   "source": [
    "Display the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bb8bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81c92cd438>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83283c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81c11ada90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input img\n",
    "plt.imshow(img.unsqueeze(0)[0,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12159c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883cae83",
   "metadata": {},
   "source": [
    "### Dectecing features with convolutions\n",
    "weight and bias are parameters that are learned through back-propagation. However, we can play with convolution by setting weights by hand and see what happens. Let's first zero out bias, just to remove any confounding factors, and then set weights to a constant value so that each pixel in the output gets the mean of its neighbors. For each $3\\times 3$ neighborhood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90ca934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388559c",
   "metadata": {},
   "source": [
    "Let's see the effect on our CIFAR image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c13d7d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f81c11246a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.imshow(output[0,0].detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73f987",
   "metadata": {},
   "source": [
    "As we can predict, the filter produces a blurred version of the image. After all, every pixel of the output is the average of a neightborhood of the input, so pixel in the output are correlated and change more smoothly.\n",
    "\n",
    "edge-dectection kernel, have already been talked about in image processing.\n",
    "\n",
    "Max pooling is provided by the nn.MaxPool2d module. It takes as input size of the neighborhood over which to operate the pooling operation. If we wish to downsample our image by half, we'll want to use a size of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db2064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "\n",
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84736d53",
   "metadata": {},
   "source": [
    "Putting it all together for our network. Let's take our previous fully connected model as a starting point and introduce nn.Conv2d and nn.MaxPool2d as described previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1746b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "                      nn.Tanh(), \n",
    "                      nn.MaxPool2d(2),nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "                      nn.Tanh(),\n",
    "                      nn.MaxPool2d(2),\n",
    "                      #\n",
    "                      nn.Linear(8 * 8 * 8, 32),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(32, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0a13b",
   "metadata": {},
   "source": [
    "Where does this end? After the input image has been reduced to a set of $8\\times 8$ features, we expect to output some probabilities from the network that we can feed to our negative log likelihood. However, probabilities are a pair of numbers in a 1D vector, but here we're still dealing with multichannel 2D features.\n",
    "\n",
    "Let's count the number of parameters for this small model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02cb291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53446607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(img.unsqueeze(0))\n",
    "\n",
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c38a7",
   "metadata": {},
   "source": [
    "What's missing here is the reshaping step from an 8-channel $8\\times 8$ image to a 512-element, 1D vector. This could be achieved by calling $view$ on the output of the last nn.MaxPool2d, but unfortunately, we don't have any explicit visibility of the output each module when we use nn.Sequential\n",
    "\n",
    "### Subclassing nn.Module\n",
    "In this section, we learn how to make our own nn.Modules subclass that we can then use just like the prebuilt ones or nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7ad3b",
   "metadata": {},
   "source": [
    "In order to subclass nn.Module, at a minimum we need to define a $forward$ function that takes the inputs to the module and returns the output. This is where we define our module's computation. The name $forward$ here is reminiscent of a distant past, when module needed to define both forward and backward passes we met. With Pytorch, if we use standard $torch$ operations, $autograd$ will take care of the backward pass automatically, and indeed, an nn.Module never comes with a $backward$.\n",
    "\n",
    "To include these submodules, we typically define them in the constructor \\_\\_init\\_\\_ and assign them to $self$ for use in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f194b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e96851c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429aa03",
   "metadata": {},
   "source": [
    "Thus, the funcitonal way also sheds light on what the nn.Module API is all about: a Module is a container for state in the forms of $Parameters$ and submodules combined with the instructions to do a forward.\n",
    "\n",
    "Now let's double-check the constructor, then we will get to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49909495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0295, -0.0812]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bee50a",
   "metadata": {},
   "source": [
    "### Training our convnet\n",
    "Recall that the core of our convnet is two nested loops: an outer one over the $epochs$ and an inner one of the $DataLoader$ that produces batches from our $Dataset$. In each loop, we have to:\n",
    "1. Feed the inputs through the model (the forward pass)\n",
    "2. Compute the loss (also the part of forward)\n",
    "3. Zero any old gradients\n",
    "4. Call loss.backward() to compute the gradients of the loss with respect to all parameters (the backward pass)\n",
    "5. Have the optimizer take a step in toward lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ee56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(),\n",
    "                                                         epoch,\n",
    "                                                         loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef8b33af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-07 15:54:30.657029 Epoch 1, Training loss 0.5747680984864569\n",
      "2021-09-07 15:54:47.666738 Epoch 10, Training loss 0.34377605349394924\n",
      "2021-09-07 15:55:06.850869 Epoch 20, Training loss 0.2979713445825941\n",
      "2021-09-07 15:55:25.740623 Epoch 30, Training loss 0.26846208893189766\n",
      "2021-09-07 15:55:44.622790 Epoch 40, Training loss 0.2478628640721558\n",
      "2021-09-07 15:56:03.803131 Epoch 50, Training loss 0.22938775921323498\n",
      "2021-09-07 15:56:23.191011 Epoch 60, Training loss 0.2131030573776573\n",
      "2021-09-07 15:56:41.829981 Epoch 70, Training loss 0.1994749170484816\n",
      "2021-09-07 15:57:00.895225 Epoch 80, Training loss 0.18499586674248336\n",
      "2021-09-07 15:57:20.425126 Epoch 90, Training loss 0.17100826195281021\n",
      "2021-09-07 15:57:41.022772 Epoch 100, Training loss 0.1582068313079275\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "training_loop(n_epochs = 100,\n",
    "              optimizer = optimizer,\n",
    "              model = model,\n",
    "              loss_fn = loss_fn,\n",
    "              train_loader = train_loader,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4355e",
   "metadata": {},
   "source": [
    "Measuring the accuracy: in order to have a measure that is more interpretable than the loss, we can take a look at our accuracies on the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4e94aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.92\n",
      "Accuracy val: 0.87\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted==labels).sum())\n",
    "        \n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "    \n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0839f",
   "metadata": {},
   "source": [
    "We about halved the number of errors on the validation set. Also, we used far fewer parameters. This is telling us that the model does a better job of generalizing its task of recognizing the subject of images from a new sample, through locality and translation invariance.\n",
    "\n",
    "Saving and loading our model: Let's save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee07f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'birds_vs_airplanes.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88783c",
   "metadata": {},
   "source": [
    "The birds_vs_airplanes.pt file now contains all the parameters of the $model$: that is, weights and biases for the two convolution modules and the two linear modules. So, no structure-just the weights. This means when we deploy the model in production for our friend, we'll need to keep the $model$ class handy, create an instance, and then load the parameters back into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ac0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()\n",
    "loaded_model.load_state_dict(torch.load(data_path+'birds_vs_airplanes.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
