{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c710df",
   "metadata": {},
   "source": [
    "Here, we will use a dataset, called CIFAR-10. It has been a computer vision classic for a decade.\n",
    "\n",
    "7.1.1 Downloading CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f18d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "data_path = '../CIFAR-10/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e75181",
   "metadata": {},
   "source": [
    "The second argument specifies whether we're interested in the training set or validation set; and the third says whether we allow Pytorch to download the data if it is not found in the location specified in the first argument.\n",
    "\n",
    "The dataset is returned as a subclass of torch.utils.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c30342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " typing.Generic,\n",
       " object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dc904",
   "metadata": {},
   "source": [
    "7.1.2 The dataset class: \n",
    "\n",
    "It's an object that is required to implement two methods:$\\_\\_len\\_\\_$ and $\\_\\_getitem\\_\\_$. The former should return the number of items in the dataset; the latter should return the item, consisting of a sample and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685a84f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ffd18",
   "metadata": {},
   "source": [
    "We can use the standard subscript for indexing tuples and lists to access individuals items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d542f41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x7F70A09BFCF8>,\n",
       " 1,\n",
       " 'automobile')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']\n",
    "\n",
    "img, label = cifar10[99]\n",
    "img, label, class_names[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d3ee2",
   "metadata": {},
   "source": [
    "So the sample in the data.CIFAR10 dataset is an instane of an RGB PIL(Python Image Library) image. We can plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b02facf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e9661",
   "metadata": {},
   "source": [
    "7.1.3 Dataset transforms:\n",
    "\n",
    "We'll likely need a way to convert the PIL image to a PyTorch tensors. The torchvision.transforms defines a set of composable, function-like objects that can be passed as an argument to a torchvision dataset such as datasets.CIFAR10(), and that perform transformation on the data after it's loaded but before it is returned by \\_\\_getitem\\_\\_. Let's see the list of available objects as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ebd28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoAugment', 'AutoAugmentPolicy', 'CenterCrop', 'ColorJitter', 'Compose']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "dir(transforms)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7b8e6",
   "metadata": {},
   "source": [
    "Among these transforms, we can spot $ToTensor$, which turns NumPy arrays and PIL images to tensors. It also takes care to lay out the dimensions of the output tensors as C\\*H\\*W (channel, height, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2787eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77509f18",
   "metadata": {},
   "source": [
    "A three channel(RGB) 32 $\\times$ 32 images. Note that nothing has happened to label; it is still an integer.\n",
    "\n",
    "As we anticipated, we can pass the transform directly as an argument to dataset.CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd64ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path,\n",
    "                                  train=True,\n",
    "                                  download=False,\n",
    "                                  transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255f31a",
   "metadata": {},
   "source": [
    "At this point, accessing an element of the dataset will return a tensor, rather than a PIL image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9f1455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d1240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f66cd",
   "metadata": {},
   "source": [
    "Whereas the values in the orginal PIL image ranged from 0 to 255, the ToTensor turns the data into a 32-bit floating-point per channel, scaling the values down from 0.0 to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42f4668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58121612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# changes the order of axes from C*H*W to H*W*C to match what matplotlib expects\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa2eab",
   "metadata": {},
   "source": [
    "7.1.4 Normalizing data\n",
    "\n",
    "We can chain them using transforms.Compose, and they can handle normalization and data augmentation transparently, directly in the data loader. For instance, it's good practice to normalize the dataset so that each channel has zero mean and unitary sd.\n",
    "\n",
    "The values of mean and stdev must be computed offline. Let's stack all the tensors returned by the dataset along an extra dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7185e6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ea4bc",
   "metadata": {},
   "source": [
    "Now we can easily compute the mean per channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe487bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613c7d5",
   "metadata": {},
   "source": [
    "Recall that view(3, -1) keeps the three channel and merges all the remaining dimension into one, figuring out the appropriate size. Here our 3\\*32\\*32 image is transformed into a 3\\*1024 vector, and then the mean is taken over the 1024 elements of each channel.\n",
    "\n",
    "Computing the stdev is similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97bf995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c198f",
   "metadata": {},
   "source": [
    "With these number in our hands, we can initialize the $Normalize$ transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0601b914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ad957",
   "metadata": {},
   "source": [
    "and concatenate it after the $ToTensor$ transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb666b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                                      transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                   transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                                        (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c080258",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                           transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                         transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                              (0.2470, 0.2435, 0.2616))]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                               transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                             transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                                                  (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b4e04",
   "metadata": {},
   "source": [
    "## 7.2 Distinguishing birds from airplanes\n",
    "7.2.1 Building the dataset\n",
    "\n",
    "We could create a $Dataset$ subclass that only includes birds and airplanes. We can take a shortcut and just filter the data in cifar10 and remap the labels so they are contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b421b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10\n",
    "          if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881e97b",
   "metadata": {},
   "source": [
    "7.2.2 A fully connected model\n",
    "\n",
    "In this demo, how many features per sample? Well, 32\\*32\\*32\\*3 = 3072 inputs features per sample. We will use linear model again :)?\n",
    "\n",
    "Our new model would be an nn.Linear with 3072 input features and some numbers of hidden numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7606c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_out = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 512,),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, n_out,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14d9d5",
   "metadata": {},
   "source": [
    "7.2.3 Output of a classifier. In the ideal case, the network would output torch.tensor([1.0, 0.0]) for an airplane and torch.tensor([0.0, 1.0]) for a bird.\n",
    "\n",
    "7.2.4 Representing the output as probabilities. Softmax is a function that takes a vector of values and produces another vector of the same dimension, where the values satisfy the constraints we just listed to represent probabilities.\n",
    "\n",
    "Let's test it on an input vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1c1969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum()\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e9a5693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304f2f13",
   "metadata": {},
   "source": [
    "We can now add a softmax at the end of our model, and our network will be equiped to produce probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1627ae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24429fd6",
   "metadata": {},
   "source": [
    "In order to call the model, we need to make the input have the right dimensions. We recall that our model expects 3072 input features, and that nn works with data organized into batch along the zeroth dimension. So we need to turn 3\\*32\\*32 image into a 1D tensor and then add an extra dimension in the zeroth position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b1658eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "img_batch = img.view(-1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "428b9e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4902, 0.5098]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img_batch)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d51c7",
   "metadata": {},
   "source": [
    "Time to get training.\n",
    "\n",
    "7.2.5 A loss for classifying: We want a loss function that is very high when the likelihood is low. There is a loss function that behaves that way, and it's called $negative\\_log\\_likelihood$ (NLL). Summing up, our loss for classification can be computed as follows. For each sample in the bacth:\n",
    "1. Run the forward pass, and obtain the output values from the last layer\n",
    "2. Compute the softmax, and obtain probabilities\n",
    "3. Take the predicted probability corresponding to the correct class. Note that we know what the correct class is because it's a superviesd problem.\n",
    "4. Compute its logarithm, slap a minus sign in front of it, and add it to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09dd627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be10f0f",
   "metadata": {},
   "source": [
    "Then we instantiate our NLL loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a5cc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6aeb314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "489e33b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7939, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img.view(-1).unsqueeze(0))\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cbffa",
   "metadata": {},
   "source": [
    "7.2.6 Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe758cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf2e4e",
   "metadata": {},
   "source": [
    "In our training code, we choose minibatches of size 1 by picking one item at a time from the dataset. The torch.utils.data module has a class that helps with shuffling and organizing the data in minibatches: Dataloader.\n",
    "\n",
    "The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose from different sampling strategies. A very common strategy is uniform sampling after shuffling the data at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d0340d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c71f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3075f2",
   "metadata": {},
   "source": [
    "we can compute the accuracy of our model on the validation set in terms of the number of correct classifications over the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f75134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.812\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4370b",
   "metadata": {},
   "source": [
    "It's quite common to drop the last nn.LogSoftmax layer from the network and use nn.CrossEntropyLoss as a loss. Let us try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eff2a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.361836\n",
      "Epoch: 1, Loss: 0.228973\n",
      "Epoch: 2, Loss: 0.298336\n",
      "Epoch: 3, Loss: 0.499869\n",
      "Epoch: 4, Loss: 0.357887\n",
      "Epoch: 5, Loss: 0.344600\n",
      "Epoch: 6, Loss: 0.178505\n",
      "Epoch: 7, Loss: 0.379637\n",
      "Epoch: 8, Loss: 0.370935\n",
      "Epoch: 9, Loss: 0.878428\n",
      "Epoch: 10, Loss: 0.950302\n",
      "Epoch: 11, Loss: 0.270932\n",
      "Epoch: 12, Loss: 0.282296\n",
      "Epoch: 13, Loss: 0.576294\n",
      "Epoch: 14, Loss: 0.950157\n",
      "Epoch: 15, Loss: 0.519083\n",
      "Epoch: 16, Loss: 0.339401\n",
      "Epoch: 17, Loss: 0.277401\n",
      "Epoch: 18, Loss: 0.448433\n",
      "Epoch: 19, Loss: 0.221876\n",
      "Epoch: 20, Loss: 0.307782\n",
      "Epoch: 21, Loss: 0.194784\n",
      "Epoch: 22, Loss: 0.235053\n",
      "Epoch: 23, Loss: 0.340276\n",
      "Epoch: 24, Loss: 0.263133\n",
      "Epoch: 25, Loss: 0.492113\n",
      "Epoch: 26, Loss: 0.148804\n",
      "Epoch: 27, Loss: 0.394775\n",
      "Epoch: 28, Loss: 0.168948\n",
      "Epoch: 29, Loss: 0.079750\n",
      "Epoch: 30, Loss: 0.559918\n",
      "Epoch: 31, Loss: 0.189379\n",
      "Epoch: 32, Loss: 0.422231\n",
      "Epoch: 33, Loss: 0.341690\n",
      "Epoch: 34, Loss: 0.101193\n",
      "Epoch: 35, Loss: 0.346187\n",
      "Epoch: 36, Loss: 0.103391\n",
      "Epoch: 37, Loss: 0.205965\n",
      "Epoch: 38, Loss: 0.101041\n",
      "Epoch: 39, Loss: 0.140645\n",
      "Epoch: 40, Loss: 0.034982\n",
      "Epoch: 41, Loss: 0.077691\n",
      "Epoch: 42, Loss: 0.134133\n",
      "Epoch: 43, Loss: 0.065795\n",
      "Epoch: 44, Loss: 0.073622\n",
      "Epoch: 45, Loss: 0.111044\n",
      "Epoch: 46, Loss: 0.956098\n",
      "Epoch: 47, Loss: 0.023810\n",
      "Epoch: 48, Loss: 0.051410\n",
      "Epoch: 49, Loss: 0.033906\n",
      "Epoch: 50, Loss: 0.030538\n",
      "Epoch: 51, Loss: 0.009636\n",
      "Epoch: 52, Loss: 0.167486\n",
      "Epoch: 53, Loss: 0.017950\n",
      "Epoch: 54, Loss: 0.017942\n",
      "Epoch: 55, Loss: 0.004892\n",
      "Epoch: 56, Loss: 0.066818\n",
      "Epoch: 57, Loss: 0.013072\n",
      "Epoch: 58, Loss: 0.029770\n",
      "Epoch: 59, Loss: 0.074475\n",
      "Epoch: 60, Loss: 0.014327\n",
      "Epoch: 61, Loss: 0.017616\n",
      "Epoch: 62, Loss: 0.021589\n",
      "Epoch: 63, Loss: 0.006798\n",
      "Epoch: 64, Loss: 0.020936\n",
      "Epoch: 65, Loss: 0.006438\n",
      "Epoch: 66, Loss: 0.003122\n",
      "Epoch: 67, Loss: 0.004257\n",
      "Epoch: 68, Loss: 0.003867\n",
      "Epoch: 69, Loss: 0.069391\n",
      "Epoch: 70, Loss: 0.002696\n",
      "Epoch: 71, Loss: 0.042014\n",
      "Epoch: 72, Loss: 0.003032\n",
      "Epoch: 73, Loss: 0.006597\n",
      "Epoch: 74, Loss: 0.001050\n",
      "Epoch: 75, Loss: 0.002474\n",
      "Epoch: 76, Loss: 0.004430\n",
      "Epoch: 77, Loss: 0.007595\n",
      "Epoch: 78, Loss: 0.001489\n",
      "Epoch: 79, Loss: 0.004294\n",
      "Epoch: 80, Loss: 0.012822\n",
      "Epoch: 81, Loss: 0.002895\n",
      "Epoch: 82, Loss: 0.001472\n",
      "Epoch: 83, Loss: 0.002332\n",
      "Epoch: 84, Loss: 0.010450\n",
      "Epoch: 85, Loss: 0.001821\n",
      "Epoch: 86, Loss: 0.006789\n",
      "Epoch: 87, Loss: 0.003118\n",
      "Epoch: 88, Loss: 0.002697\n",
      "Epoch: 89, Loss: 0.001190\n",
      "Epoch: 90, Loss: 0.002477\n",
      "Epoch: 91, Loss: 0.000657\n",
      "Epoch: 92, Loss: 0.005677\n",
      "Epoch: 93, Loss: 0.000224\n",
      "Epoch: 94, Loss: 0.005347\n",
      "Epoch: 95, Loss: 0.000486\n",
      "Epoch: 96, Loss: 0.005468\n",
      "Epoch: 97, Loss: 0.001635\n",
      "Epoch: 98, Loss: 0.000306\n",
      "Epoch: 99, Loss: 0.001955\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "042bb7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.808000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d269b1c",
   "metadata": {},
   "source": [
    "Pytorch offers a quick way to determine how many parameters a model has through the $parameters()$ method of $nn.Model$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46eb81be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3737474, [3145728, 1024, 524288, 512, 65536, 128, 256, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_list = [p.numel() \n",
    "              for p in model.parameters() \n",
    "              if p.requires_grad==True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd519e",
   "metadata": {},
   "source": [
    "Let's check the quantity of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35827973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8919f",
   "metadata": {},
   "source": [
    "Our neural network won't scale very well with the number of pixels. What if we had a $1024\\times 1024$ RGB image? That's 3.1 million inputs values. Even abruptly going to 1024 hidden features, we would have over 3 billion parameters. Using 32-bit floats, we're already at 12GB RAM, and we haven't even hit the second layer, much less computed and stored the gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
